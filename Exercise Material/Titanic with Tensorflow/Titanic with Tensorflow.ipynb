{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMYQvJuBi7MS"
   },
   "source": [
    "# Titanic with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nna1tOKxyEqe"
   },
   "source": [
    "This tutorial demonstrates how to classify structured data, such as tabular data, using the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/overview) dataset. In large parts it is based on the Tensorflow Tutorial [Classify structured data using Keras preprocessing layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjFbdBldyEqf"
   },
   "source": [
    "## Import TensorFlow and other libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:06.193791Z",
     "iopub.status.busy": "2021-11-06T01:29:06.193189Z",
     "iopub.status.idle": "2021-11-06T01:29:07.959771Z",
     "shell.execute_reply": "2021-11-06T01:29:07.959238Z"
    },
    "id": "LklnLlt6yEqf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:07.968923Z",
     "iopub.status.busy": "2021-11-06T01:29:07.968348Z",
     "iopub.status.idle": "2021-11-06T01:29:07.971695Z",
     "shell.execute_reply": "2021-11-06T01:29:07.972083Z"
    },
    "id": "TKU7RyoQGVKB"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXvBvobayEqi"
   },
   "source": [
    "## Load the dataset and read it into a pandas DataFrame\n",
    "\n",
    "<a href=\"https://pandas.pydata.org/\" class=\"external\">pandas</a> is a Python library with many helpful utilities for loading and working with structured data. Use `tf.keras.utils.get_file` to download and extract the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:07.976355Z",
     "iopub.status.busy": "2021-11-06T01:29:07.975751Z",
     "iopub.status.idle": "2021-11-06T01:29:08.621259Z",
     "shell.execute_reply": "2021-11-06T01:29:08.620731Z"
    },
    "id": "qJ4Ajn-YyEqj"
   },
   "outputs": [],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/daka1510/hhz-artificial-intelligence-vl-s22/main/Exercise%20Material/Titanic%20with%20Tensorflow/train.csv\"\n",
    "csv_file = tf.keras.utils.get_file(\"train.csv\", dataset_url, cache_dir=\".\")\n",
    "dataframe = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efa6910dfa5f"
   },
   "source": [
    "Inspect the dataset by checking the first five rows of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:08.635916Z",
     "iopub.status.busy": "2021-11-06T01:29:08.630289Z",
     "iopub.status.idle": "2021-11-06T01:29:08.638914Z",
     "shell.execute_reply": "2021-11-06T01:29:08.638473Z"
    },
    "id": "3uiq4hoIGyXI"
   },
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3zDbrozyEqq"
   },
   "source": [
    "## Prepare data\n",
    "\n",
    "Data preparation for this dataset was covered in depth in another notebook see [Titanic Data Preparation](https://github.com/daka1510/hhz-artificial-intelligence-vl-s22/blob/main/Exercise%20Material/Titanic/Data%20Preparation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:08.645129Z",
     "iopub.status.busy": "2021-11-06T01:29:08.644527Z",
     "iopub.status.idle": "2021-11-06T01:29:08.648921Z",
     "shell.execute_reply": "2021-11-06T01:29:08.648463Z"
    },
    "id": "wmMDc46-yEqq"
   },
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column 'Cabin' since there are too many missing values\n",
    "dataframe = dataframe.drop([\"Cabin\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values for 'Embarked': use most frequent value\n",
    "dataframe[\"Embarked\"] = dataframe[\"Embarked\"].fillna(\n",
    "    dataframe[\"Embarked\"].mode().iloc[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values for 'Age': use mean value\n",
    "dataframe[\"Age\"] = dataframe[\"Age\"].fillna((dataframe[\"Age\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify results\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp0NCbswyEqs"
   },
   "source": [
    "## Split the DataFrame into training, validation, and test sets\n",
    "\n",
    "The dataset is in a single pandas DataFrame. Split it into training, validation, and test sets using a, for example, 80:10:10 ratio, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:08.653752Z",
     "iopub.status.busy": "2021-11-06T01:29:08.653124Z",
     "iopub.status.idle": "2021-11-06T01:29:08.660033Z",
     "shell.execute_reply": "2021-11-06T01:29:08.660426Z"
    },
    "id": "XvSinthO8oMj"
   },
   "outputs": [],
   "source": [
    "train, val, test = np.split(\n",
    "    dataframe.sample(frac=1), [int(0.8 * len(dataframe)), int(0.9 * len(dataframe))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:08.665320Z",
     "iopub.status.busy": "2021-11-06T01:29:08.664671Z",
     "iopub.status.idle": "2021-11-06T01:29:08.667267Z",
     "shell.execute_reply": "2021-11-06T01:29:08.667679Z"
    },
    "id": "U02Q1moWoPwQ"
   },
   "outputs": [],
   "source": [
    "print(len(train), \"training examples\")\n",
    "print(len(val), \"validation examples\")\n",
    "print(len(test), \"test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_7uVu-xyEqv"
   },
   "source": [
    "## Create an input pipeline using tf.data\n",
    "\n",
    "Next, create a utility function that converts each training, validation, and test set DataFrame into a `tf.data.Dataset`, then shuffles and batches the data.\n",
    "\n",
    "Note: If you were working with a very large CSV file (so large that it does not fit into memory), you would use the `tf.data` API to read it from disk directly. That is not covered in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:08.675156Z",
     "iopub.status.busy": "2021-11-06T01:29:08.674507Z",
     "iopub.status.idle": "2021-11-06T01:29:08.676219Z",
     "shell.execute_reply": "2021-11-06T01:29:08.676657Z"
    },
    "id": "7r4j-1lRyEqw"
   },
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop(\"Survived\")\n",
    "    df = {key: value[:, tf.newaxis] for key, value in dataframe.items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYxIXH579uS9"
   },
   "source": [
    "Now, use the newly created function (`df_to_dataset`) to check the format of the data the input pipeline helper function returns by calling it on the training data, and use a small batch size to keep the output readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:08.681455Z",
     "iopub.status.busy": "2021-11-06T01:29:08.680588Z",
     "iopub.status.idle": "2021-11-06T01:29:10.255881Z",
     "shell.execute_reply": "2021-11-06T01:29:10.256247Z"
    },
    "id": "tYiNH-QI96Jo"
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:10.260994Z",
     "iopub.status.busy": "2021-11-06T01:29:10.260424Z",
     "iopub.status.idle": "2021-11-06T01:29:10.340694Z",
     "shell.execute_reply": "2021-11-06T01:29:10.341105Z"
    },
    "id": "nFYir6S8HgIJ"
   },
   "outputs": [],
   "source": [
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print(\"Every feature:\", list(train_features.keys()))\n",
    "print(\"A batch of ages:\", train_features[\"Age\"])\n",
    "print(\"A batch of targets:\", label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geqHWW54Hmte"
   },
   "source": [
    "As the output demonstrates, the training set returns a dictionary of column names (from the DataFrame) that map to column values from rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v50jBIuj4gb"
   },
   "source": [
    "## Apply the Keras preprocessing layers\n",
    "\n",
    "The Keras preprocessing layers allow you to build Keras-native input processing pipelines, which can be used as independent preprocessing code in non-Keras workflows, combined directly with Keras models, and exported as part of a Keras SavedModel.\n",
    "\n",
    "In this tutorial, you will use the following four preprocessing layers to demonstrate how to perform preprocessing, structured data encoding, and feature engineering:\n",
    "\n",
    "- `tf.keras.layers.Normalization`: Performs feature-wise normalization of input features.\n",
    "- `tf.keras.layers.CategoryEncoding`: Turns integer categorical features into one-hot, multi-hot, or <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf/\" class=\"external\">tf-idf</a>\n",
    "dense representations.\n",
    "- `tf.keras.layers.StringLookup`: Turns string categorical values into integer indices.\n",
    "- `tf.keras.layers.IntegerLookup`: Turns integer categorical values into integer indices.\n",
    "\n",
    "You can learn more about the available layers in the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide.\n",
    "\n",
    "- For _numerical features_, you will use a `tf.keras.layers.Normalization` layer to standardize the distribution of the data.\n",
    "- For _categorical features_, you will transform them to multi-hot encoded tensors with `tf.keras.layers.CategoryEncoding`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twXBSxnT66o8"
   },
   "source": [
    "### Numerical columns\n",
    "\n",
    "For each numeric feature, you will use a `tf.keras.layers.Normalization` layer to standardize the distribution of the data.\n",
    "\n",
    "Define a new utility function that returns a layer which applies feature-wise normalization to numerical features using that Keras preprocessing layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:10.346096Z",
     "iopub.status.busy": "2021-11-06T01:29:10.345460Z",
     "iopub.status.idle": "2021-11-06T01:29:10.347171Z",
     "shell.execute_reply": "2021-11-06T01:29:10.347551Z"
    },
    "id": "D6OuEKMMyEq1"
   },
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for the feature.\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "    # Prepare a Dataset that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL4TRreQCPjV"
   },
   "source": [
    "Next, test the new function by calling it on the age feature to normalize `'Age'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:10.351495Z",
     "iopub.status.busy": "2021-11-06T01:29:10.350922Z",
     "iopub.status.idle": "2021-11-06T01:29:12.356462Z",
     "shell.execute_reply": "2021-11-06T01:29:12.356873Z"
    },
    "id": "MpKgUDyk69bM"
   },
   "outputs": [],
   "source": [
    "age_col = train_features[\"Age\"]\n",
    "layer = get_normalization_layer(\"Age\", train_ds)\n",
    "layer(age_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWY00YBUx9N"
   },
   "source": [
    "Note: If you have many numeric features (hundreds, or more), it is more efficient to concatenate them first and use a single `tf.keras.layers.Normalization` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVD--2WZ7vmh"
   },
   "source": [
    "### Categorical columns\n",
    "\n",
    "The `Sex` and `Embarked` columns in the dataset are represented as strings which need to be multi-hot encoded before being fed into the model.\n",
    "\n",
    "Define another new utility function that returns a layer which maps values from a vocabulary to integer indices and multi-hot encodes the features using the `tf.keras.layers.StringLookup`, `tf.keras.layers.IntegerLookup`, and `tf.keras.CategoryEncoding` preprocessing layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:12.363367Z",
     "iopub.status.busy": "2021-11-06T01:29:12.362455Z",
     "iopub.status.idle": "2021-11-06T01:29:12.364617Z",
     "shell.execute_reply": "2021-11-06T01:29:12.364991Z"
    },
    "id": "GmgaeRjlDoUO"
   },
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a layer that turns strings into integer indices.\n",
    "    if dtype == \"string\":\n",
    "        index = layers.StringLookup(max_tokens=max_tokens)\n",
    "    # Otherwise, create a layer that turns integer values into integer indices.\n",
    "    else:\n",
    "        index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "    # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "\n",
    "    # Encode the integer indices.\n",
    "    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "    # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "    # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "    return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b3DwtTeCPjX"
   },
   "source": [
    "Test the `get_category_encoding_layer` function by calling it on `'Embarked'` features to turn them into multi-hot encoded tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:12.459598Z",
     "iopub.status.busy": "2021-11-06T01:29:12.458584Z",
     "iopub.status.idle": "2021-11-06T01:29:14.130980Z",
     "shell.execute_reply": "2021-11-06T01:29:14.130484Z"
    },
    "id": "X2t2ff9K8PcT"
   },
   "outputs": [],
   "source": [
    "test_type_col = train_features[\"Embarked\"]\n",
    "test_type_layer = get_category_encoding_layer(\n",
    "    name=\"Embarked\", dataset=train_ds, dtype=\"string\"\n",
    ")\n",
    "test_type_layer(test_type_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process on the `'Sex'` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_type_col = train_features[\"Sex\"]\n",
    "test_type_layer = get_category_encoding_layer(\n",
    "    name=\"Sex\", dataset=train_ds, dtype=\"string\"\n",
    ")\n",
    "test_type_layer(test_type_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiE0glOPkMyh"
   },
   "source": [
    "## Preprocess selected features to train the model on\n",
    "\n",
    "You have learned how to use several types of Keras preprocessing layers. Next, you will:\n",
    "\n",
    "- Apply the preprocessing utility functions defined earlier\n",
    "- Add all the feature inputs to a list.\n",
    "\n",
    "Note: If your aim is to build an accurate model, try a larger dataset of your own, and think carefully about which features are the most meaningful to include, and how they should be represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj1GoHSZ9R3H"
   },
   "source": [
    "Earlier, you used a small batch size to demonstrate the input pipeline. Let's now create a new input pipeline with a larger batch size of 256:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:16.302923Z",
     "iopub.status.busy": "2021-11-06T01:29:16.301922Z",
     "iopub.status.idle": "2021-11-06T01:29:16.326887Z",
     "shell.execute_reply": "2021-11-06T01:29:16.326342Z"
    },
    "id": "Rcv2kQTTo23h"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bIGNYN2V7iR"
   },
   "source": [
    "Normalize the numerical features, and add them to one list of inputs called `encoded_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:16.333431Z",
     "iopub.status.busy": "2021-11-06T01:29:16.332404Z",
     "iopub.status.idle": "2021-11-06T01:29:16.862202Z",
     "shell.execute_reply": "2021-11-06T01:29:16.861621Z"
    },
    "id": "Q3RBa51VkaAn"
   },
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# Numerical features.\n",
    "for header in [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]:\n",
    "    numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(header, train_ds)\n",
    "    encoded_numeric_col = normalization_layer(numeric_col)\n",
    "    all_inputs.append(numeric_col)\n",
    "    encoded_features.append(encoded_numeric_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVcUAFd6bvlT"
   },
   "source": [
    "Turn the integer categorical values from the dataset into integer indices, perform multi-hot encoding, and add the resulting feature inputs to `encoded_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:16.869537Z",
     "iopub.status.busy": "2021-11-06T01:29:16.865714Z",
     "iopub.status.idle": "2021-11-06T01:29:17.071184Z",
     "shell.execute_reply": "2021-11-06T01:29:17.070623Z"
    },
    "id": "1FOMGfZflhoA"
   },
   "outputs": [],
   "source": [
    "pclass_col = tf.keras.Input(shape=(1,), name=\"Pclass\", dtype=\"int64\")\n",
    "\n",
    "encoding_layer = get_category_encoding_layer(\n",
    "    name=\"Pclass\", dataset=train_ds, dtype=\"int64\", max_tokens=5\n",
    ")\n",
    "encoded_pclass_col = encoding_layer(pclass_col)\n",
    "all_inputs.append(pclass_col)\n",
    "encoded_features.append(encoded_pclass_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYzynk6wdqKe"
   },
   "source": [
    "Repeat the same step for the string categorical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:17.075474Z",
     "iopub.status.busy": "2021-11-06T01:29:17.074832Z",
     "iopub.status.idle": "2021-11-06T01:29:18.996614Z",
     "shell.execute_reply": "2021-11-06T01:29:18.996111Z"
    },
    "id": "K8C8xyiXm-Ie"
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\"Sex\", \"Embarked\"]\n",
    "\n",
    "for header in categorical_cols:\n",
    "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype=\"string\")\n",
    "    encoding_layer = get_category_encoding_layer(\n",
    "        name=header, dataset=train_ds, dtype=\"string\", max_tokens=5\n",
    "    )\n",
    "    encoded_categorical_col = encoding_layer(categorical_col)\n",
    "    all_inputs.append(categorical_col)\n",
    "    encoded_features.append(encoded_categorical_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHSnhz2fyEq3"
   },
   "source": [
    "## Create, compile, and train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDGyN_wpo0XS"
   },
   "source": [
    "The next step is to create a model using the [Keras Functional API](https://www.tensorflow.org/guide/keras/functional). For the first layer in your model, merge the list of feature inputs—`encoded_features`—into one vector via concatenation with `tf.keras.layers.concatenate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:19.000642Z",
     "iopub.status.busy": "2021-11-06T01:29:19.000071Z",
     "iopub.status.idle": "2021-11-06T01:29:19.031917Z",
     "shell.execute_reply": "2021-11-06T01:29:19.032250Z"
    },
    "id": "6Yrj-_pr6jyL"
   },
   "outputs": [],
   "source": [
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRLDRcYAefTA"
   },
   "source": [
    "Configure the model with Keras `Model.compile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:19.038374Z",
     "iopub.status.busy": "2021-11-06T01:29:19.037777Z",
     "iopub.status.idle": "2021-11-06T01:29:19.045331Z",
     "shell.execute_reply": "2021-11-06T01:29:19.045697Z"
    },
    "id": "GZDb_lJdelSg"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6mNMfG6yEq5"
   },
   "source": [
    "Optional: Let's visualize the connectivity graph (requires pyplot and graphviz to be installed). See [model.png](https://github.com/daka1510/hhz-artificial-intelligence-vl-22/blob/main/Exercise%20Material/Titanic%20with%20Tensorflow/model.png) for an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:19.050303Z",
     "iopub.status.busy": "2021-11-06T01:29:19.049714Z",
     "iopub.status.idle": "2021-11-06T01:29:19.571188Z",
     "shell.execute_reply": "2021-11-06T01:29:19.571553Z"
    },
    "id": "Y7Bkx4c7yEq5"
   },
   "outputs": [],
   "source": [
    "# Use `rankdir='LR'` to make the graph horizontal.\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CED6OStLyEq7"
   },
   "source": [
    "Next, train and test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:19.578679Z",
     "iopub.status.busy": "2021-11-06T01:29:19.578140Z",
     "iopub.status.idle": "2021-11-06T01:29:24.780906Z",
     "shell.execute_reply": "2021-11-06T01:29:24.781309Z"
    },
    "id": "OQfE3PC6yEq8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=50, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:24.785593Z",
     "iopub.status.busy": "2021-11-06T01:29:24.784989Z",
     "iopub.status.idle": "2021-11-06T01:29:24.833536Z",
     "shell.execute_reply": "2021-11-06T01:29:24.833055Z"
    },
    "id": "T8N2uAdU2Cni"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, \"darkgreen\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"darkblue\", label=\"Validation accuracy\")\n",
    "plt.plot(epochs, loss, \"lightgreen\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"lightblue\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model (Confusion Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_categories = tf.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_categories = tf.concat([y for x, y in test_ds], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yields count of true negatives, false positives, false negatives, true positives\n",
    "confusion_matrix(predicted_categories, true_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that tp, fp, tn, fn are not confused\n",
    "tn, fp, fn, tp = confusion_matrix(true_categories, predicted_categories).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yields class-specific precision, recall and f1-score\n",
    "print(classification_report(true_categories, predicted_categories)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmZMnTKaCZda"
   },
   "source": [
    "## Perform inference\n",
    "\n",
    "The model you have developed can now classify a row from a CSV file directly after you've included the preprocessing layers inside the model itself.\n",
    "\n",
    "You can now [save and reload the Keras model](../keras/save_and_load.ipynb) with `Model.save` and `Model.load_model` before performing inference on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:24.839842Z",
     "iopub.status.busy": "2021-11-06T01:29:24.839253Z",
     "iopub.status.idle": "2021-11-06T01:29:30.110385Z",
     "shell.execute_reply": "2021-11-06T01:29:30.110751Z"
    },
    "id": "QH9Zy1sBvwOH"
   },
   "outputs": [],
   "source": [
    "model.save(\"titanic_classifier/1\")\n",
    "reloaded_model = tf.keras.models.load_model(\"titanic_classifier/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D973plJrdwQ9"
   },
   "source": [
    "To get a prediction for a new sample, you can simply call the Keras `Model.predict` method. There are just two things you need to do:\n",
    "\n",
    "1.   Wrap scalars into a list so as to have a batch dimension (`Model`s only process batches of data, not single samples).\n",
    "2.   Call `tf.convert_to_tensor` on each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T01:29:30.124401Z",
     "iopub.status.busy": "2021-11-06T01:29:30.123735Z",
     "iopub.status.idle": "2021-11-06T01:29:30.556432Z",
     "shell.execute_reply": "2021-11-06T01:29:30.555965Z"
    },
    "id": "rKq4pxtdDa7i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"PassengerId\": 231,\n",
    "    \"Pclass\": 3,\n",
    "    \"Name\": \"Braund, Mr. Owen Harris\",\n",
    "    \"Sex\": \"male\",\n",
    "    \"Age\": 22.0,\n",
    "    \"SibSp\": 1,\n",
    "    \"Parch\": 0,\n",
    "    \"Ticket\": \"A/5 21171\",\n",
    "    \"Fare\": 7.25,\n",
    "    \"Embarked\": \"S\",\n",
    "}\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "prob = reloaded_model.predict(input_dict)[0]\n",
    "print(\n",
    "    \"This particular passenger had a %.1f percent probability \"\n",
    "    \"of surviving.\" % (100 * prob)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJQQZEiH2FaB"
   },
   "source": [
    "Note: You will typically have better results with deep learning with larger and more complex datasets. When working with a small dataset, simpler algorithms (e.g. decision tree, logistic regression) often provide a strong baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/docker) to serve your model with the aid of a Docker container. Requires a local [docker](https://www.docker.com/) installation. To try it out, run below commands in a Terminal (and replace local paths as needed):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 1 Download the TensorFlow Serving Docker image\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "# 2 Start TensorFlow Serving container and open the REST API port\n",
    "docker run -t --rm -p 8501:8501 \\\n",
    "    -v \"/Users/dkaulen/Development/daka1510/hhz-artificial-intelligence-vl-w21/Exercise Material/Titanic with Tensorflow/titanic_classifier:/models/titanic_classifier\" \\\n",
    "    -e MODEL_NAME=titanic_classifier \\\n",
    "    tensorflow/serving\n",
    "\n",
    "# 3 Check model metadata\n",
    "curl http://localhost:8501/v1/models/titanic_classifier/metadata\n",
    "\n",
    "# 4 Perform inference via the REST API (using the supported subset of features)\n",
    "curl -d '{\"instances\": [{\"Pclass\":[3],\"Sex\":[\"male\"],\"Age\":[22.0],\"SibSp\":[1],\"Parch\":[0],\"Fare\":[7.25],\"Embarked\":[\"S\"]}]}' -X POST http://localhost:8501/v1/models/titanic_classifier:predict\n",
    "\n",
    "# 5 Get list of running containers \n",
    "docker ps\n",
    "\n",
    "# 6 Stop running container forcefully\n",
    "docker rm -f <id>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing_layers.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
